{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# libraries\n",
      "%pylab inline\n",
      "import gzip, cPickle\n",
      "import numpy as np\n",
      "from random import shuffle\n",
      "\n",
      "# methods\n",
      "def load_mnist():\n",
      "\tf = gzip.open('mnist.pkl.gz', 'rb')\n",
      "\tdata = cPickle.load(f)\n",
      "\tf.close()\n",
      "\treturn data\n",
      "\n",
      "\n",
      "def plot_digits(data, numcols, shape=(28,28)):\n",
      "    numdigits = data.shape[0]\n",
      "    numrows = int(numdigits/numcols)\n",
      "    for i in range(numdigits):\n",
      "        plt.subplot(numrows, numcols, i)\n",
      "        plt.axis('off')\n",
      "        plt.imshow(data[i].reshape(shape), interpolation='nearest', cmap='Greys')\n",
      "    plt.show()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1.1 Gradient-based stochastic optimization\n",
      "## 1.1.1 Derive gradient computation equations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "$$\\mathcal{L}=\\sum\\limits_{n=1}^N\\mathcal{L}^{(n)}$$\n",
      "and \n",
      "$$\\mathcal{L}^{(n)}=\\ln(p(t=t_n \\;|\\; \\textbf{x}_n, \\textbf{b}, \\textbf{W})>0$$\n",
      "Hence, we can optimize $\\mathcal{L}$ for  every  data point separately.\n",
      "$$\\mathcal{L}^{(n)}\n",
      "=\\ln(p(t=t_n \\;|\\; \\textbf{x}_n, \\textbf{b}, \\textbf{W})\n",
      "=\\ln(q_n)-\\ln(Z)=x_n\\textbf{w}_{t_n}^T+b_{t_n}-\\ln(\\sum\\limits_j \\exp\\{x_n\\textbf{w}_{j}^T+b_{j}\\})$$\n",
      "\n",
      "\n",
      "\n",
      "$$ \\dfrac{\\partial\\mathcal{L}^{(n)} }{\\partial b_{i}}= \\delta_{t_ni}-\\dfrac{\\partial Z }{\\partial b_i}\\dfrac{\\partial \\ln(Z) }{\\partial Z}\n",
      "=\\delta_{t_ni}- \\dfrac{\\exp\\{\\textbf{x}_n^{T}\\textbf{w}_i+b_i\\}}{Z}$$\n",
      "\n",
      "$$ \\dfrac{\\partial\\mathcal{L}^{(n)}}{\\partial \\textbf{w}_i}\n",
      "= \\delta_{t_ni}\\textbf{x}_n-\\dfrac{\\partial Z }{\\partial \\textbf{w}_i}\\dfrac{\\partial \\ln(Z) }{\\partial Z}\n",
      "=\\delta_{t_ni}\\textbf{x}_n- \\dfrac{\\textbf{x}_n\\exp\\{\\textbf{x}_n^{T}\\textbf{w}_i+b_i\\}}{Z}\n",
      "=\\textbf{x}_n\\cdot\\dfrac{\\partial\\mathcal{L}^{(n)} }{\\partial b_i} $$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1.1.2 Compute gradients of the log-likelihood"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logreg_gradient(x, t, W, b):\n",
      "    M,J=W.shape\n",
      "    N=x.shape\n",
      "    grad_b=zeros(J)\n",
      "    grad_W=zeros((M,J))\n",
      "    # compute Z\n",
      "    Z=0\n",
      "    for j in xrange(J):\n",
      "        Z+=exp(np.dot(x,W[:,j].T)+b[j])\n",
      "    # compute grad_b\n",
      "    for j in xrange(J):\n",
      "        grad_b[j]=-exp(np.dot(x,W[:,j].T)+b[j])/Z\n",
      "    grad_b[t]+=1\n",
      "    # compute grad_W\n",
      "    for j in xrange(J):\n",
      "        grad_W[:,j]=x*grad_b[j]\n",
      "        \n",
      "    return grad_b, grad_W"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1.1.3 Stochastic gradient descent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sgd_iter(x_train, t_train, W, b):\n",
      "    M,J=W.shape\n",
      "    N,M=x_train.shape\n",
      "    eta=1E-4 #learning_rate\n",
      "    indices = np.arange(N,  dtype = int)\n",
      "    np.random.shuffle(indices)\n",
      "\n",
      "    # stochatic gradient decent\n",
      "    for n in indices:\n",
      "        grad_b, grad_W = logreg_gradient(x_train[n,:],t_train[n],W,b)\n",
      "        b+= eta*grad_b \n",
      "        W+= eta*grad_W\n",
      "    return W,b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1.2 Train\n",
      "\n",
      "## 1.2.1 Train"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load data\n",
      "(x_train, t_train), (x_valid, t_valid), (x_test, t_test) = load_mnist()\n",
      "#initilaize W and b\n",
      "N,M=x_train.shape\n",
      "J=10\n",
      "b=zeros(J)#ask about that\n",
      "W=zeros((M,J))\n",
      "\n",
      "# some iterations\n",
      "for i in xrange(20):\n",
      "    W,b=sgd_iter(x_train, t_train, W, b)\n",
      "    # prediction with x_test and t_test\n",
      "    # calculate error_test\n",
      "    # prediction with x_valid and t_valid\n",
      "    # calculate error_valid\n",
      "    # plot error of training/ validation\n",
      "    \n",
      "    # Visulaze W, W[:,j] is one image\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1.2.2 Visualize weights\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.2.3. Visualize the 8 hardest and 8 easiest digits\n",
      "Visualize the 8 digits in the validation set with the highest probability of the true class label under the model. Also plot the 8 digits that\n",
      "were assigned the lowest probability. Ask yourself if these results make sense.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2. Multilayer perceptron\n",
      "$$\\mathcal{L}^{(n)}\n",
      "=\\ln(p(t=t_n \\;|\\; \\textbf{x}_n, \\textbf{b}, \\textbf{W})\n",
      "=\\ln(q_n)-\\ln(Z)=\\textbf{w}_{t_n}^T\\textbf{h}^{(n)}+b_{t_n}-\\ln(\\sum\\limits_j \\exp\\{x_n\\textbf{w}_{j}^T+b_{j}\\})$$\n",
      "\n",
      "$$h_l=\\sigma(\\textbf{v}_l^T\\textbf{x}_n+a_l)$$\n",
      "##2.1 Derive gradient equations\n",
      "\n",
      "In Part one our feature vector $\\phi(\\textbf{x})$ was equal to $\\textbf{x}$. Now we have the slightly more complicated case that\n",
      "$$\\phi(\\textbf{x})=\\textbf{h}(\\textbf{x}).$$\n",
      "However, in both cases $\\phi(\\textbf{x})$ is only a parameter when deriving $\\nabla_{\\textbf{b}} \\mathcal{L}^{(n)}$ and $\\nabla_{\\textbf{W}} \\mathcal{L}^{(n)}$. Thus, both terms are similarly to part 1 computed as follows:\n",
      "$$ \\dfrac{\\partial\\mathcal{L}^{(n)} }{\\partial b_{i}}= \\delta_{t_ni}-\\dfrac{\\partial Z }{\\partial b_i}\\dfrac{\\partial \\ln(Z) }{\\partial Z}\n",
      "=\\delta_{t_ni}- \\dfrac{\\exp\\{\\textbf{x}_n^{T}\\textbf{w}_i+b_i\\}}{Z}$$\n",
      "\n",
      "$$ \\dfrac{\\partial\\mathcal{L}^{(n)}}{\\partial \\textbf{w}_i}\n",
      "= \\delta_{t_ni}\\textbf{h}_n-\\dfrac{\\partial Z }{\\partial \\textbf{w}_i}\\dfrac{\\partial \\ln(Z) }{\\partial Z}\n",
      "=\\delta_{t_ni}\\textbf{h}_n- \\dfrac{\\textbf{x}_n\\exp\\{\\textbf{x}_n^{T}\\textbf{w}_i+b_i\\}}{Z}\n",
      "=\\textbf{h}^{(n)}\\cdot\\dfrac{\\partial\\mathcal{L}^{(n)} }{\\partial b_i} $$\n",
      "\n",
      "\n",
      "In the case of $\\textbf{a}$ and $\\textbf{V}$. We will have to have a closer look at the model.\n",
      "\n",
      "$$ \\dfrac{\\partial\\mathcal{L}^{(n)}}{\\partial a_l}\n",
      "= \\dfrac{\\partial\\mathcal{L}^{(n)}}{\\partial h^{(n)}_l}\\dfrac{\\partial h^{(n)}_l}{\\partial a_l}\n",
      "=\\delta^h_l \\dfrac{\\partial h^{(n)}_l}{\\partial r}\\dfrac{\\partial r}{\\partial a_l}\n",
      "=\\delta^h_l [\\sigma(r)(1-\\sigma(r))][1]\n",
      "=\\delta^h_l [\\sigma(\\textbf{v}_l^T\\textbf{x}_n+a_l)(1-\\sigma(\\textbf{v}_l^T\\textbf{x}_n+a_l))]\n",
      "$$\n",
      "\n",
      "\n",
      "$$ \\dfrac{\\partial\\mathcal{L}^{(n)}}{\\partial \\textbf{v}_l}\n",
      "= \\dfrac{\\partial\\mathcal{L}^{(n)}}{\\partial h^{(n)}_l}\\dfrac{\\partial h^{(n)}_l}{\\partial \\textbf{v}_l}\n",
      "=\\delta^h_l \\dfrac{\\partial h^{(n)}_l}{\\partial r}\\dfrac{\\partial r}{\\partial \\textbf{v}_l}\n",
      "=\\delta^h_l [\\sigma(r)(1-\\sigma(r))][\\textbf{x}_n]\n",
      "=\\delta^h_l \\textbf{x}_n [\\sigma(\\textbf{v}_l^T\\textbf{x}_n+a_l)(1-\\sigma(\\textbf{v}_l^T\\textbf{x}_n+a_l))]\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}